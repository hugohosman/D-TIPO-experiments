{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f322acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Example 1: Basic Mean-Variance Optimization\n",
      "============================================================\n",
      "Configuration saved to results/basic_mv/config.json\n",
      "Theoretical values:\n",
      "  ExT: 1.2000\n",
      "  VarxT: 0.032836\n",
      "  Lambda: 1.104013\n",
      "  Optimal loss: -1.163748\n",
      "Initializing model: GBM_MV\n",
      "Initializing model: GBM_MV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training for 8 epochs...\n",
      "Epoch 1/8\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/opt/anaconda3/envs/vscode-env/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/anaconda3/envs/vscode-env/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        \n    File \"/opt/anaconda3/envs/vscode-env/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        \n    File \"/opt/anaconda3/envs/vscode-env/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1150, in train_step\n        \n    File \"/opt/anaconda3/envs/vscode-env/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        \n    File \"/var/folders/_8/_0j0hf_n6pq4pp5n6q844f7m0000gq/T/__autograph_generated_fileflyedb8u.py\", line 21, in tf__call\n        P = ag__.converted_call(ag__.ld(np).ones, ([ag__.ld(M), ag__.ld(self).d, ag__.ld(self).N + 1],), None, fscope)\n    File \"/opt/anaconda3/envs/vscode-env/lib/python3.11/site-packages/numpy/core/numeric.py\", line 191, in ones\n        a = empty(shape, dtype, order)\n\n    TypeError: Exception encountered when calling layer 'improved_full_network_1' (type ImprovedFullNetwork).\n    \n    in user code:\n    \n        File \"/Users/willemijnvandervaart/Documents/Total_refactor_D_TIPO/experiment_training_improved.py\", line 165, in call  *\n            P = np.ones([M, self.d, self.N+1])\n        File \"/opt/anaconda3/envs/vscode-env/lib/python3.11/site-packages/numpy/core/numeric.py\", line 191, in ones  **\n            a = empty(shape, dtype, order)\n    \n        TypeError: 'SymbolicTensor' object cannot be interpreted as an integer\n    \n    \n    Call arguments received by layer 'improved_full_network_1' (type ImprovedFullNetwork):\n      • x_in=tf.Tensor(shape=(16384, 1), dtype=float32)\n      • training=True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 286\u001b[39m\n\u001b[32m    282\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Training epochs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(results[\u001b[33m'\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 250\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    247\u001b[39m os.makedirs(\u001b[33m'\u001b[39m\u001b[33m./results\u001b[39m\u001b[33m'\u001b[39m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    249\u001b[39m \u001b[38;5;66;03m# Run examples\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m results_basic = \u001b[43mexample_1_basic_mean_variance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m results_jumps = example_2_with_jumps()\n\u001b[32m    252\u001b[39m \u001b[38;5;66;03m# results_options = example_3_with_options()  # Uncomment if FullNetwork supports options\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mexample_1_basic_mean_variance\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     28\u001b[39m config = ExperimentConfig(\n\u001b[32m     29\u001b[39m     model=ModelConfig(\n\u001b[32m     30\u001b[39m         model_name=\u001b[33m'\u001b[39m\u001b[33mGBM_MV\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     54\u001b[39m     )\n\u001b[32m     55\u001b[39m )\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Run experiment\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m results = \u001b[43mexperiment_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./results/basic_mv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Plot results\u001b[39;00m\n\u001b[32m     61\u001b[39m plot_experiment_results(results, save_path=\u001b[33m\"\u001b[39m\u001b[33m./results/basic_mv/summary.png\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Total_refactor_D_TIPO/experiment_training_improved.py:391\u001b[39m, in \u001b[36mexperiment_training\u001b[39m\u001b[34m(config, save_dir, verbose)\u001b[39m\n\u001b[32m    388\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStarting training for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.training.num_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m epochs...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    390\u001b[39m zero_vec = np.zeros(x0.shape)\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m history = \u001b[43mfull_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mzero_vec\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m    398\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[38;5;66;03m# Save training history if save directory is specified\u001b[39;00m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m save_dir:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/vscode-env/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     67\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/var/folders/_8/_0j0hf_n6pq4pp5n6q844f7m0000gq/T/__autograph_generated_file6oiurbkl.py:15\u001b[39m, in \u001b[36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     14\u001b[39m     do_return = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(\u001b[38;5;28mself\u001b[39m), ag__.ld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m     17\u001b[39m     do_return = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/var/folders/_8/_0j0hf_n6pq4pp5n6q844f7m0000gq/T/__autograph_generated_fileflyedb8u.py:21\u001b[39m, in \u001b[36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[39m\u001b[34m(self, x_in, training)\u001b[39m\n\u001b[32m     19\u001b[39m S = ag__.converted_call(ag__.ld(tf).ones, ([ag__.ld(batch_size), ag__.ld(\u001b[38;5;28mself\u001b[39m).d],), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[32m     20\u001b[39m S_Q = ag__.converted_call(ag__.ld(tf).ones, ([ag__.ld(batch_size), ag__.ld(\u001b[38;5;28mself\u001b[39m).d],), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m P = \u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mones\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mN\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m delta_N_acu = \u001b[32m0.0\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_state_4\u001b[39m():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/vscode-env/lib/python3.11/site-packages/numpy/core/numeric.py:191\u001b[39m, in \u001b[36mones\u001b[39m\u001b[34m(shape, dtype, order, like)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m like \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    189\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _ones_with_like(like, shape, dtype=dtype, order=order)\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m a = empty(shape, dtype, order)\n\u001b[32m    192\u001b[39m multiarray.copyto(a, \u001b[32m1\u001b[39m, casting=\u001b[33m'\u001b[39m\u001b[33munsafe\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[31mTypeError\u001b[39m: in user code:\n\n    File \"/opt/anaconda3/envs/vscode-env/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/anaconda3/envs/vscode-env/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        \n    File \"/opt/anaconda3/envs/vscode-env/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        \n    File \"/opt/anaconda3/envs/vscode-env/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1150, in train_step\n        \n    File \"/opt/anaconda3/envs/vscode-env/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        \n    File \"/var/folders/_8/_0j0hf_n6pq4pp5n6q844f7m0000gq/T/__autograph_generated_fileflyedb8u.py\", line 21, in tf__call\n        P = ag__.converted_call(ag__.ld(np).ones, ([ag__.ld(M), ag__.ld(self).d, ag__.ld(self).N + 1],), None, fscope)\n    File \"/opt/anaconda3/envs/vscode-env/lib/python3.11/site-packages/numpy/core/numeric.py\", line 191, in ones\n        a = empty(shape, dtype, order)\n\n    TypeError: Exception encountered when calling layer 'improved_full_network_1' (type ImprovedFullNetwork).\n    \n    in user code:\n    \n        File \"/Users/willemijnvandervaart/Documents/Total_refactor_D_TIPO/experiment_training_improved.py\", line 165, in call  *\n            P = np.ones([M, self.d, self.N+1])\n        File \"/opt/anaconda3/envs/vscode-env/lib/python3.11/site-packages/numpy/core/numeric.py\", line 191, in ones  **\n            a = empty(shape, dtype, order)\n    \n        TypeError: 'SymbolicTensor' object cannot be interpreted as an integer\n    \n    \n    Call arguments received by layer 'improved_full_network_1' (type ImprovedFullNetwork):\n      • x_in=tf.Tensor(shape=(16384, 1), dtype=float32)\n      • training=True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Example usage of the refactored D-TIPO experiment framework\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from experiment_training_improved import (\n",
    "    ExperimentConfig, ModelConfig, NetworkConfig, \n",
    "    TrainingConfig, TradingConfig, experiment_training,\n",
    "    plot_experiment_results\n",
    ")\n",
    "\n",
    "# Reload the module to pick up the fix\n",
    "import importlib\n",
    "import model_classes_refactored\n",
    "importlib.reload(model_classes_refactored)\n",
    "\n",
    "\n",
    "def example_1_basic_mean_variance():\n",
    "    \"\"\"\n",
    "    Example 1: Basic mean-variance optimization without options\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Example 1: Basic Mean-Variance Optimization\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create configuration\n",
    "    config = ExperimentConfig(\n",
    "        model=ModelConfig(\n",
    "            model_name='GBM_MV',\n",
    "            T=2.0,\n",
    "            d=5,\n",
    "            r=0.06,\n",
    "            ExT=1.2,\n",
    "            jumps=False\n",
    "        ),\n",
    "        network=NetworkConfig(\n",
    "            regularization=0.0,\n",
    "            neurons=[20, 20],\n",
    "            activation='relu'\n",
    "        ),\n",
    "        training=TrainingConfig(\n",
    "            num_epochs=8,\n",
    "            batch_size=2**14,  # Smaller for faster demo\n",
    "            learning_rate=0.01,\n",
    "            M=2**18,  # Smaller for faster demo\n",
    "            N=20\n",
    "        ),\n",
    "        trading=TradingConfig(\n",
    "            curtage=0.005,\n",
    "            leverage_constraints=False,\n",
    "            options_in_p=False,\n",
    "            bankrupcy_constraint=False\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Run experiment\n",
    "    results = experiment_training(config, save_dir=\"./results/basic_mv\")\n",
    "    \n",
    "    # Plot results\n",
    "    plot_experiment_results(results, save_path=\"./results/basic_mv/summary.png\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def example_2_with_jumps():\n",
    "    \"\"\"\n",
    "    Example 2: Portfolio optimization with jump processes\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Example 2: Portfolio Optimization with Jumps\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    config = ExperimentConfig(\n",
    "        model=ModelConfig(\n",
    "            model_name='GBM_MV',\n",
    "            T=2.0,\n",
    "            d=5,\n",
    "            r=0.06,\n",
    "            ExT=1.2,\n",
    "            jumps=True,  # Enable jumps\n",
    "            lamb=0.05,   # Jump intensity\n",
    "            mu=0.0,      # Jump mean\n",
    "            s=0.2        # Jump std dev\n",
    "        ),\n",
    "        network=NetworkConfig(\n",
    "            regularization=0.0,\n",
    "            neurons=[20, 20],\n",
    "            activation='relu'\n",
    "        ),\n",
    "        training=TrainingConfig(\n",
    "            num_epochs=10,  # More epochs for complex dynamics\n",
    "            batch_size=2**14,\n",
    "            learning_rate=0.01,\n",
    "            M=2**18,\n",
    "            N=20\n",
    "        ),\n",
    "        trading=TradingConfig(\n",
    "            curtage=0.005,\n",
    "            leverage_constraints=False,\n",
    "            options_in_p=False,\n",
    "            bankrupcy_constraint=True  # Enable bankruptcy constraint\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    results = experiment_training(config, save_dir=\"./results/with_jumps\")\n",
    "    plot_experiment_results(results, save_path=\"./results/with_jumps/summary.png\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def example_3_with_options():\n",
    "    \"\"\"\n",
    "    Example 3: D-TIPO with options (full implementation)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Example 3: D-TIPO with Options\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    config = ExperimentConfig(\n",
    "        model=ModelConfig(\n",
    "            model_name='GBM_MV',\n",
    "            T=2.0,\n",
    "            d=5,\n",
    "            r=0.06,\n",
    "            ExT=1.2,\n",
    "            jumps=True,\n",
    "            lamb=0.05,\n",
    "            mu=0.0,\n",
    "            s=0.2\n",
    "        ),\n",
    "        network=NetworkConfig(\n",
    "            regularization=1e-6,  # Add regularization\n",
    "            neurons=[20, 20, 20],  # Deeper network\n",
    "            activation='relu'\n",
    "        ),\n",
    "        training=TrainingConfig(\n",
    "            num_epochs=15,  # More training for options\n",
    "            batch_size=2**14,\n",
    "            learning_rate=0.01,\n",
    "            M=2**18,\n",
    "            N=20\n",
    "        ),\n",
    "        trading=TradingConfig(\n",
    "            curtage=0.005,\n",
    "            leverage_constraints=True,   # Enable leverage constraints\n",
    "            options_in_p=True,          # Enable options trading\n",
    "            bankrupcy_constraint=True\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    results = experiment_training(config, save_dir=\"./results/with_options\")\n",
    "    plot_experiment_results(results, save_path=\"./results/with_options/summary.png\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def example_4_custom_parameters():\n",
    "    \"\"\"\n",
    "    Example 4: Custom drift and volatility parameters\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Example 4: Custom Market Parameters\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Custom drift vector (declining returns)\n",
    "    custom_b = tf.reshape(tf.linspace(0.10, 0.02, 5), [5, 1])\n",
    "    \n",
    "    # Custom volatility matrix with correlations\n",
    "    custom_sigma = tf.constant([\n",
    "        [0.20, 0.05, 0.02, 0.01, 0.01],\n",
    "        [0.05, 0.18, 0.04, 0.02, 0.01],\n",
    "        [0.02, 0.04, 0.16, 0.03, 0.02],\n",
    "        [0.01, 0.02, 0.03, 0.14, 0.04],\n",
    "        [0.01, 0.01, 0.02, 0.04, 0.12]\n",
    "    ])\n",
    "    \n",
    "    config = ExperimentConfig(\n",
    "        model=ModelConfig(\n",
    "            model_name='GBM_MV',\n",
    "            T=3.0,  # Longer time horizon\n",
    "            d=5,\n",
    "            r=0.04,  # Lower risk-free rate\n",
    "            ExT=1.15,  # Lower expected terminal wealth\n",
    "            b=custom_b,\n",
    "            sigma=custom_sigma,\n",
    "            jumps=False\n",
    "        ),\n",
    "        network=NetworkConfig(\n",
    "            regularization=0.0,\n",
    "            neurons=[30, 30],  # Larger network\n",
    "            activation='relu'\n",
    "        ),\n",
    "        training=TrainingConfig(\n",
    "            num_epochs=12,\n",
    "            batch_size=2**15,\n",
    "            learning_rate=0.005,  # Lower learning rate\n",
    "            M=2**19,\n",
    "            N=30  # More rebalancing dates\n",
    "        ),\n",
    "        trading=TradingConfig(\n",
    "            curtage=0.002,  # Lower transaction costs\n",
    "            leverage_constraints=False,\n",
    "            options_in_p=False,\n",
    "            bankrupcy_constraint=False\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    results = experiment_training(config, save_dir=\"./results/custom_params\")\n",
    "    plot_experiment_results(results, save_path=\"./results/custom_params/summary.png\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def compare_experiments(results_list, labels):\n",
    "    \"\"\"\n",
    "    Compare multiple experiment results\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for results, label in zip(results_list, labels):\n",
    "        train_loss = results['train_loss']\n",
    "        plt.plot(train_loss, '.-', linewidth=2, label=label)\n",
    "    \n",
    "    # Add theoretical optimal line from first experiment\n",
    "    optimal_loss = results_list[0]['theoretical_values']['optimal_loss']\n",
    "    plt.axhline(y=optimal_loss, color='k', linestyle='--', \n",
    "                label=f'Theoretical Optimal: {optimal_loss:.4f}')\n",
    "    \n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Comparison of Different Configurations')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('./results/comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Run all examples and compare results\n",
    "    \"\"\"\n",
    "    # Create results directory\n",
    "    import os\n",
    "    os.makedirs('./results', exist_ok=True)\n",
    "    \n",
    "    # Run examples\n",
    "    results_basic = example_1_basic_mean_variance()\n",
    "    results_jumps = example_2_with_jumps()\n",
    "    # results_options = example_3_with_options()  # Uncomment if FullNetwork supports options\n",
    "    results_custom = example_4_custom_parameters()\n",
    "    \n",
    "    # Compare experiments\n",
    "    compare_experiments(\n",
    "        [results_basic, results_jumps, results_custom],\n",
    "        ['Basic MV', 'With Jumps', 'Custom Parameters']\n",
    "    )\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Summary of All Experiments\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    experiments = [\n",
    "        ('Basic MV', results_basic),\n",
    "        ('With Jumps', results_jumps),\n",
    "        ('Custom Parameters', results_custom)\n",
    "    ]\n",
    "    \n",
    "    for name, results in experiments:\n",
    "        final_loss = results['train_loss'][-1]\n",
    "        optimal_loss = results['theoretical_values']['optimal_loss']\n",
    "        gap = abs(final_loss - optimal_loss)\n",
    "        \n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Final Loss: {final_loss:.6f}\")\n",
    "        print(f\"  Optimal Loss: {optimal_loss:.6f}\")\n",
    "        print(f\"  Gap: {gap:.6f} ({gap/abs(optimal_loss)*100:.2f}%)\")\n",
    "        print(f\"  Lambda: {results['theoretical_values']['lam']:.6f}\")\n",
    "        print(f\"  Training epochs: {len(results['train_loss'])}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "mrqn2qb9dog",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'null' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Let's run the notebook to see the current error\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/Users/willemijnvandervaart/Documents/Total_refactor_D_TIPO/example_usage.ipynb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:5\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'null' is not defined"
     ]
    }
   ],
   "source": [
    "# Let's run the notebook to see the current error\n",
    "exec(open('/Users/willemijnvandervaart/Documents/Total_refactor_D_TIPO/example_usage.ipynb').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nabozno19s",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and run the example to see the current error\n",
    "import sys\n",
    "sys.path.append('/Users/willemijnvandervaart/Documents/Total_refactor_D_TIPO')\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from experiment_training_improved import (\n",
    "    ExperimentConfig, ModelConfig, NetworkConfig, \n",
    "    TrainingConfig, TradingConfig, experiment_training,\n",
    "    plot_experiment_results\n",
    ")\n",
    "\n",
    "# Reload the module to pick up the fix\n",
    "import importlib\n",
    "import model_classes_refactored\n",
    "importlib.reload(model_classes_refactored)\n",
    "\n",
    "def example_1_basic_mean_variance():\n",
    "    \"\"\"\n",
    "    Example 1: Basic mean-variance optimization without options\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Example 1: Basic Mean-Variance Optimization\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create configuration\n",
    "    config = ExperimentConfig(\n",
    "        model=ModelConfig(\n",
    "            model_name='GBM_MV',\n",
    "            T=2.0,\n",
    "            d=5,\n",
    "            r=0.06,\n",
    "            ExT=1.2,\n",
    "            jumps=False\n",
    "        ),\n",
    "        network=NetworkConfig(\n",
    "            regularization=0.0,\n",
    "            neurons=[20, 20],\n",
    "            activation='relu'\n",
    "        ),\n",
    "        training=TrainingConfig(\n",
    "            num_epochs=2,  # Reduced for testing\n",
    "            batch_size=2**10,  # Smaller for testing\n",
    "            learning_rate=0.01,\n",
    "            M=2**12,  # Smaller for testing\n",
    "            N=5  # Smaller for testing\n",
    "        ),\n",
    "        trading=TradingConfig(\n",
    "            curtage=0.005,\n",
    "            leverage_constraints=False,\n",
    "            options_in_p=False,\n",
    "            bankrupcy_constraint=False\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Run experiment\n",
    "    results = experiment_training(config, save_dir=\"./results/basic_mv\")\n",
    "    return results\n",
    "\n",
    "# Try to run the example\n",
    "try:\n",
    "    results = example_1_basic_mean_variance()\n",
    "    print(\"Success!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {type(e).__name__}: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
